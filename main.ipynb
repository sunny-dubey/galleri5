{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b56fa-9ca2-454c-82e1-dc58b2b12e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Save current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Clone the repo if it doesn't exist\n",
    "if not os.path.exists(\"DiffSynth-Studio\"):\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/modelscope/DiffSynth-Studio.git\"])\n",
    "\n",
    "# Change into the repo directory\n",
    "os.chdir(\"DiffSynth-Studio\")\n",
    "\n",
    "# Install in editable mode using the current Python environment\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\"])\n",
    "\n",
    "# Go back to original directory\n",
    "os.chdir(cwd)\n",
    "\n",
    "print(\"âœ… DiffSynth-Studio installed and returned to original directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7446c-1b4b-4693-a76a-e6a771a78281",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate --upgrade\n",
    "!pip install -e DiffSynth-Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ac87d-21eb-43c2-a878-48cb1f53ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch DiffSynth-Studio/examples/qwen_image/model_training/train.py \\\n",
    "  --dataset_base_path data/ranveer_singh \\\n",
    "  --dataset_metadata_path data/ranveer_singh/metadata.json \\\n",
    "  --data_file_keys \"image\" \\\n",
    "  --max_pixels 1048576 \\\n",
    "  --model_id_with_origin_paths \"Qwen/Qwen-Image-Edit-2509:text_encoder/model*.safetensors,Qwen/Qwen-Image-Edit-2509:vae/diffusion_pytorch_model.safetensors\" \\\n",
    "  --output_path \"./models/ranveer_cache\" \\\n",
    "  --task data_process \\\n",
    "  --dataset_num_workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c82fe-f608-438a-a9e9-a506d3546c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "import os  \n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "!accelerate launch DiffSynth-Studio/examples/qwen_image/model_training/train.py \\\n",
    "  --dataset_base_path models/ranveer_cache \\\n",
    "  --max_pixels 786432 \\\n",
    "  --dataset_repeat 15 \\\n",
    "  --model_id_with_origin_paths \"Qwen/Qwen-Image-Edit-2509:transformer/diffusion_pytorch_model*.safetensors\" \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --num_epochs 5 \\\n",
    "  --remove_prefix_in_ckpt \"pipe.dit.\" \\\n",
    "  --output_path \"./models/train/ranveer_lora\" \\\n",
    "  --lora_base_model \"dit\" \\\n",
    "  --lora_target_modules \"to_q,to_k,to_v,add_q_proj,add_k_proj,add_v_proj,to_out.0,to_add_out,img_mlp.net.2,img_mod.1,txt_mlp.net.2,txt_mod.1\" \\\n",
    "  --lora_rank 64 \\\n",
    "  --use_gradient_checkpointing \\\n",
    "  --use_gradient_checkpointing_offload \\\n",
    "  --gradient_accumulation_steps 8 \\\n",
    "  --dataset_num_workers 8 \\\n",
    "  --find_unused_parameters \\\n",
    "  --enable_fp8_training \\\n",
    "  --task sft \\\n",
    "  --save_steps 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53f8cc-9722-4f3d-8531-795f7c54a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd DiffSynth-Studio && pip install -e .\n",
    "!pip install -e DiffSynth-Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b790046-931e-4119-bd02-e3c9b5a9c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89947157-ac30-44d9-a886-ed170c321517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "import sys, os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'  \n",
    "sys.path.append(os.path.abspath(\"./DiffSynth-Studio\"))\n",
    "from diffsynth.pipelines.qwen_image import QwenImagePipeline, ModelConfig  \n",
    "from PIL import Image  \n",
    "import torch  \n",
    "  \n",
    "# Load the base model with processor from ModelScope  \n",
    "pipe = QwenImagePipeline.from_pretrained(  \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    device=\"cuda\",  \n",
    "    model_configs=[  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image-Edit-2509\", origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image-Edit-2509\", origin_file_pattern=\"text_encoder/model*.safetensors\"),  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image-Edit-2509\", origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),  \n",
    "    ],  \n",
    "    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image-Edit-2509\", origin_file_pattern=\"tokenizer/\"),  \n",
    "    processor_config=None  \n",
    ")  \n",
    "  \n",
    "# Load processor from a Qwen2.5-VL model that has processor files  \n",
    "from transformers import Qwen2VLProcessor  \n",
    "pipe.processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", trust_remote_code=True) \n",
    "  \n",
    "# # Load your trained LoRA  \n",
    "# pipe.load_lora(pipe.dit, \"./models/train/ranveer_lora/step-100.safetensors\")  \n",
    "  \n",
    "# # Edit an image  \n",
    "# image = pipe(  \n",
    "#     prompt=\"Replace the person with Ranveer Singh\",  \n",
    "#     edit_image=Image.open(\"images.jpg\"),  \n",
    "#     seed=42,  \n",
    "#     num_inference_steps=40  \n",
    "# )    \n",
    "# image.save(\"test_output1.jpg\")\n",
    "\n",
    "\n",
    "# Path to LoRA checkpoints\n",
    "lora_dir = \"./models/train/ranveer_lora\"\n",
    "\n",
    "# Load edit image once\n",
    "edit_image = Image.open(\"rahul.jpeg\")\n",
    "\n",
    "# Loop through all .safetensors files in lora_dir\n",
    "for file_name in sorted(os.listdir(lora_dir)):\n",
    "    if file_name.endswith(\".safetensors\"):\n",
    "        lora_path = os.path.join(lora_dir, file_name)\n",
    "        print(f\"ðŸ”¹ Testing LoRA: {file_name}\")\n",
    "\n",
    "        # Load LoRA weights\n",
    "        pipe.load_lora(pipe.dit, lora_path, alpha=1.5)\n",
    "\n",
    "        output = pipe(\n",
    "            prompt=\"Replace the person face with Ranveer Singh\",\n",
    "            edit_image=edit_image,\n",
    "            cfg_scale=9.0,\n",
    "            seed=42,\n",
    "            num_inference_steps=60\n",
    "        )\n",
    "\n",
    "        # Save with unique filename\n",
    "        output_path = f\"rahul_outputs/{os.path.splitext(file_name)[0]}_output.jpg\"\n",
    "        os.makedirs(\"rahul_outputs\", exist_ok=True)\n",
    "        output.save(output_path)\n",
    "\n",
    "        print(f\"âœ… Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b498ca-67ee-41ba-8ba6-57daa659cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "import sys, os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'  \n",
    "from diffsynth.pipelines.qwen_image import QwenImagePipeline, ModelConfig  \n",
    "from PIL import Image  \n",
    "import torch  \n",
    "  \n",
    "# Load base Qwen-Image model  \n",
    "pipe = QwenImagePipeline.from_pretrained(  \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    device=\"cuda\",  \n",
    "    model_configs=[  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\",   \n",
    "                   origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\",   \n",
    "                   origin_file_pattern=\"text_encoder/model*.safetensors\"),  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\",   \n",
    "                   origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),  \n",
    "    ],  \n",
    "    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image\",   \n",
    "                                 origin_file_pattern=\"tokenizer/\"),  \n",
    "    # Note: No processor_config needed for base model  \n",
    ")  \n",
    "  \n",
    "# Load your trained LoRA  \n",
    "pipe.load_lora(pipe.dit, \"./models/train/ranveer_lora/step-4000.safetensors\", alpha=1.5)  \n",
    "  \n",
    "# Generate new image  \n",
    "output = pipe(  \n",
    "    prompt=\"Ranveer Singh in a mahabharat\",  \n",
    "    cfg_scale=7.5,  # Base model uses default cfg_scale=4.0  \n",
    "    seed=42,  \n",
    "    num_inference_steps=40,  # Base model uses default 30 steps  \n",
    "    height=1328,  \n",
    "    width=1328  \n",
    ")  \n",
    "output.save(\"ranveer_singh_in_mahabharat.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5162b-0a6d-4c72-92d6-a10c431965cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
