{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b56fa-9ca2-454c-82e1-dc58b2b12e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Save current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Clone the repo if it doesn't exist\n",
    "if not os.path.exists(\"DiffSynth-Studio\"):\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/modelscope/DiffSynth-Studio.git\"])\n",
    "\n",
    "# Change into the repo directory\n",
    "os.chdir(\"DiffSynth-Studio\")\n",
    "\n",
    "# Install in editable mode using the current Python environment\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\"])\n",
    "\n",
    "# Go back to original directory\n",
    "os.chdir(cwd)\n",
    "\n",
    "print(\"âœ… DiffSynth-Studio installed and returned to original directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7446c-1b4b-4693-a76a-e6a771a78281",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate --upgrade\n",
    "!pip install -e DiffSynth-Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ac87d-21eb-43c2-a878-48cb1f53ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch DiffSynth-Studio/examples/qwen_image/model_training/train.py \\\n",
    "  --dataset_base_path data/ranveer_singh \\\n",
    "  --dataset_metadata_path data/ranveer_singh/metadata.json \\\n",
    "  --data_file_keys \"image\" \\\n",
    "  --max_pixels 1048576 \\\n",
    "  --model_id_with_origin_paths \"Qwen/Qwen-Image-Edit-2509:text_encoder/model*.safetensors,Qwen/Qwen-Image-Edit-2509:vae/diffusion_pytorch_model.safetensors\" \\\n",
    "  --output_path \"./models/ranveer_cache\" \\\n",
    "  --task data_process \\\n",
    "  --dataset_num_workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54c82fe-f608-438a-a9e9-a506d3546c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "No metadata_path. Searching for cached data files.\n",
      "103 cached data files found.\n",
      "Downloading Model from https://www.modelscope.cn to directory: /workspace/models/Qwen/Qwen-Image-Edit-2509\n",
      "Loading models from: ['./models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00005-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00003-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00004-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00002-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00001-of-00005.safetensors']\n",
      "    model_name: qwen_image_dit model_class: QwenImageDiT\n",
      "    The following models are loaded: ['qwen_image_dit'].\n",
      "No qwen_image_text_encoder models available.\n",
      "Using qwen_image_dit from ['./models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00005-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00003-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00004-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00002-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00001-of-00005.safetensors'].\n",
      "No qwen_image_vae models available.\n",
      "No qwen_image_blockwise_controlnet models available.\n",
      "Downloading Model from https://www.modelscope.cn to directory: /workspace/models/Qwen/Qwen-Image-Edit\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1545/1545 [18:30<00:00,  1.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1545/1545 [17:49<00:00,  1.44it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1545/1545 [18:58<00:00,  1.36it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1545/1545 [19:42<00:00,  1.31it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1545/1545 [20:21<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch,gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "import os  \n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "!accelerate launch DiffSynth-Studio/examples/qwen_image/model_training/train.py \\\n",
    "  --dataset_base_path models/ranveer_cache \\\n",
    "  --max_pixels 786432 \\\n",
    "  --dataset_repeat 15 \\\n",
    "  --model_id_with_origin_paths \"Qwen/Qwen-Image-Edit-2509:transformer/diffusion_pytorch_model*.safetensors\" \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --num_epochs 5 \\\n",
    "  --remove_prefix_in_ckpt \"pipe.dit.\" \\\n",
    "  --output_path \"./models/train/ranveer_lora\" \\\n",
    "  --lora_base_model \"dit\" \\\n",
    "  --lora_target_modules \"to_q,to_k,to_v,add_q_proj,add_k_proj,add_v_proj,to_out.0,to_add_out,img_mlp.net.2,img_mod.1,txt_mlp.net.2,txt_mod.1\" \\\n",
    "  --lora_rank 64 \\\n",
    "  --use_gradient_checkpointing \\\n",
    "  --use_gradient_checkpointing_offload \\\n",
    "  --gradient_accumulation_steps 8 \\\n",
    "  --dataset_num_workers 8 \\\n",
    "  --find_unused_parameters \\\n",
    "  --enable_fp8_training \\\n",
    "  --task sft \\\n",
    "  --save_steps 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53f8cc-9722-4f3d-8531-795f7c54a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd DiffSynth-Studio && pip install -e .\n",
    "!pip install -e DiffSynth-Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b790046-931e-4119-bd02-e3c9b5a9c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89947157-ac30-44d9-a886-ed170c321517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /workspace/models/Qwen/Qwen-Image-Edit-2509\n",
      "Loading models from: ['./models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00005-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00003-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00004-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00002-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00001-of-00005.safetensors']\n",
      "    model_name: qwen_image_dit model_class: QwenImageDiT\n",
      "    The following models are loaded: ['qwen_image_dit'].\n",
      "Downloading Model from https://www.modelscope.cn to directory: /workspace/models/Qwen/Qwen-Image-Edit-2509\n",
      "Loading models from: ['./models/Qwen/Qwen-Image-Edit-2509/text_encoder/model-00004-of-00004.safetensors', './models/Qwen/Qwen-Image-Edit-2509/text_encoder/model-00003-of-00004.safetensors', './models/Qwen/Qwen-Image-Edit-2509/text_encoder/model-00001-of-00004.safetensors', './models/Qwen/Qwen-Image-Edit-2509/text_encoder/model-00002-of-00004.safetensors']\n",
      "    model_name: qwen_image_text_encoder model_class: QwenImageTextEncoder\n",
      "    The following models are loaded: ['qwen_image_text_encoder'].\n",
      "Downloading Model from https://www.modelscope.cn to directory: /workspace/models/Qwen/Qwen-Image-Edit-2509\n",
      "Loading models from: ./models/Qwen/Qwen-Image-Edit-2509/vae/diffusion_pytorch_model.safetensors\n",
      "    model_name: qwen_image_vae model_class: QwenImageVAE\n",
      "    The following models are loaded: ['qwen_image_vae'].\n",
      "Using qwen_image_text_encoder from ['./models/Qwen/Qwen-Image-Edit-2509/text_encoder/model-00004-of-00004.safetensors', './models/Qwen/Qwen-Image-Edit-2509/text_encoder/model-00003-of-00004.safetensors', './models/Qwen/Qwen-Image-Edit-2509/text_encoder/model-00001-of-00004.safetensors', './models/Qwen/Qwen-Image-Edit-2509/text_encoder/model-00002-of-00004.safetensors'].\n",
      "Using qwen_image_dit from ['./models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00005-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00003-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00004-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00002-of-00005.safetensors', './models/Qwen/Qwen-Image-Edit-2509/transformer/diffusion_pytorch_model-00001-of-00005.safetensors'].\n",
      "Using qwen_image_vae from ./models/Qwen/Qwen-Image-Edit-2509/vae/diffusion_pytorch_model.safetensors.\n",
      "No qwen_image_blockwise_controlnet models available.\n",
      "Downloading Model from https://www.modelscope.cn to directory: /workspace/models/Qwen/Qwen-Image-Edit-2509\n",
      "ðŸ”¹ Testing LoRA: step-2000.safetensors\n",
      "720 tensors are updated by LoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:14<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: rahul_outputs/step-2000_output.jpg\n",
      "ðŸ”¹ Testing LoRA: step-4000.safetensors\n",
      "720 tensors are updated by LoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:14<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: rahul_outputs/step-4000_output.jpg\n",
      "ðŸ”¹ Testing LoRA: step-6000.safetensors\n",
      "720 tensors are updated by LoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:14<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: rahul_outputs/step-6000_output.jpg\n",
      "ðŸ”¹ Testing LoRA: step-7725.safetensors\n",
      "720 tensors are updated by LoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 9/60 [00:11<01:05,  1.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Load LoRA weights\u001b[39;00m\n\u001b[32m     55\u001b[39m pipe.load_lora(pipe.dit, lora_path, alpha=\u001b[32m1.5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m output = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReplace the image with Ranveer Singh\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43medit_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43medit_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\n\u001b[32m     63\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Save with unique filename\u001b[39;00m\n\u001b[32m     66\u001b[39m output_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrahul_outputs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.splitext(file_name)[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_output.jpg\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DiffSynth-Studio/diffsynth/pipelines/qwen_image.py:432\u001b[39m, in \u001b[36mQwenImagePipeline.__call__\u001b[39m\u001b[34m(self, prompt, negative_prompt, cfg_scale, input_image, denoising_strength, inpaint_mask, inpaint_blur_size, inpaint_blur_sigma, height, width, seed, rand_device, num_inference_steps, exponential_shift_mu, blockwise_controlnet_inputs, eligen_entity_prompts, eligen_entity_masks, eligen_enable_on_negative, edit_image, edit_image_auto_resize, edit_rope_interpolation, context_image, enable_fp8_attention, tiled, tile_size, tile_stride, progress_bar_cmd)\u001b[39m\n\u001b[32m    429\u001b[39m timestep = timestep.unsqueeze(\u001b[32m0\u001b[39m).to(dtype=\u001b[38;5;28mself\u001b[39m.torch_dtype, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m noise_pred_posi = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs_shared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs_posi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cfg_scale != \u001b[32m1.0\u001b[39m:\n\u001b[32m    434\u001b[39m     noise_pred_nega = \u001b[38;5;28mself\u001b[39m.model_fn(**models, **inputs_shared, **inputs_nega, timestep=timestep, progress_id=progress_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DiffSynth-Studio/diffsynth/pipelines/qwen_image.py:836\u001b[39m, in \u001b[36mmodel_fn_qwen_image\u001b[39m\u001b[34m(dit, blockwise_controlnet, latents, timestep, prompt_emb, prompt_emb_mask, height, width, blockwise_controlnet_conditioning, blockwise_controlnet_inputs, progress_id, num_inference_steps, entity_prompt_emb, entity_prompt_emb_mask, entity_masks, edit_latents, context_latents, enable_fp8_attention, use_gradient_checkpointing, use_gradient_checkpointing_offload, edit_rope_interpolation, **kwargs)\u001b[39m\n\u001b[32m    832\u001b[39m     blockwise_controlnet_conditioning = blockwise_controlnet.preprocess(\n\u001b[32m    833\u001b[39m         blockwise_controlnet_inputs, blockwise_controlnet_conditioning)\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block_id, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dit.transformer_blocks):\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m     text, image = \u001b[43mgradient_checkpoint_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_gradient_checkpointing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_gradient_checkpointing_offload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconditioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_fp8_attention\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_fp8_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    847\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m blockwise_controlnet_conditioning \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    848\u001b[39m         image_slice = image[:, :image_seq_len].clone()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DiffSynth-Studio/diffsynth/vram_management/gradient_checkpointing.py:33\u001b[39m, in \u001b[36mgradient_checkpoint_forward\u001b[39m\u001b[34m(model, use_gradient_checkpointing, use_gradient_checkpointing_offload, *args, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m     model_output = torch.utils.checkpoint.checkpoint(\n\u001b[32m     27\u001b[39m         create_custom_forward(model),\n\u001b[32m     28\u001b[39m         *args,\n\u001b[32m     29\u001b[39m         **kwargs,\n\u001b[32m     30\u001b[39m         use_reentrant=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     31\u001b[39m     )\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     model_output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DiffSynth-Studio/diffsynth/models/qwen_image_dit.py:379\u001b[39m, in \u001b[36mQwenImageTransformerBlock.forward\u001b[39m\u001b[34m(self, image, text, temb, image_rotary_emb, attention_mask, enable_fp8_attention)\u001b[39m\n\u001b[32m    376\u001b[39m txt_normed = \u001b[38;5;28mself\u001b[39m.txt_norm1(text)\n\u001b[32m    377\u001b[39m txt_modulated, txt_gate = \u001b[38;5;28mself\u001b[39m._modulate(txt_normed, txt_mod_attn)\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m img_attn_out, txt_attn_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_modulated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtxt_modulated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_fp8_attention\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_fp8_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m image = image + img_gate * img_attn_out\n\u001b[32m    388\u001b[39m text = text + txt_gate * txt_attn_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DiffSynth-Studio/diffsynth/models/qwen_image_dit.py:296\u001b[39m, in \u001b[36mQwenDoubleStreamAttention.forward\u001b[39m\u001b[34m(self, image, text, image_rotary_emb, attention_mask, enable_fp8_attention)\u001b[39m\n\u001b[32m    293\u001b[39m txt_v = rearrange(txt_v, \u001b[33m'\u001b[39m\u001b[33mb s (h d) -> b h s d\u001b[39m\u001b[33m'\u001b[39m, h=\u001b[38;5;28mself\u001b[39m.num_heads)\n\u001b[32m    295\u001b[39m img_q, img_k = \u001b[38;5;28mself\u001b[39m.norm_q(img_q), \u001b[38;5;28mself\u001b[39m.norm_k(img_k)\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m txt_q, txt_k = \u001b[38;5;28mself\u001b[39m.norm_added_q(txt_q), \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_added_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image_rotary_emb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m     img_freqs, txt_freqs = image_rotary_emb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DiffSynth-Studio/diffsynth/models/sd3_dit.py:19\u001b[39m, in \u001b[36mRMSNorm.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m     18\u001b[39m     input_dtype = hidden_states.dtype\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     variance = \u001b[43mhidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m.square().mean(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     20\u001b[39m     hidden_states = hidden_states * torch.rsqrt(variance + \u001b[38;5;28mself\u001b[39m.eps)\n\u001b[32m     21\u001b[39m     hidden_states = hidden_states.to(input_dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "import sys, os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'  \n",
    "sys.path.append(os.path.abspath(\"./DiffSynth-Studio\"))\n",
    "from diffsynth.pipelines.qwen_image import QwenImagePipeline, ModelConfig  \n",
    "from PIL import Image  \n",
    "import torch  \n",
    "  \n",
    "# Load the base model with processor from ModelScope  \n",
    "pipe = QwenImagePipeline.from_pretrained(  \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    device=\"cuda\",  \n",
    "    model_configs=[  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image-Edit-2509\", origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image-Edit-2509\", origin_file_pattern=\"text_encoder/model*.safetensors\"),  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image-Edit-2509\", origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),  \n",
    "    ],  \n",
    "    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image-Edit-2509\", origin_file_pattern=\"tokenizer/\"),  \n",
    "    processor_config=None  \n",
    ")  \n",
    "  \n",
    "# Load processor from a Qwen2.5-VL model that has processor files  \n",
    "from transformers import Qwen2VLProcessor  \n",
    "pipe.processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", trust_remote_code=True) \n",
    "  \n",
    "# # Load your trained LoRA  \n",
    "# pipe.load_lora(pipe.dit, \"./models/train/ranveer_lora/step-100.safetensors\")  \n",
    "  \n",
    "# # Edit an image  \n",
    "# image = pipe(  \n",
    "#     prompt=\"Replace the person with Ranveer Singh\",  \n",
    "#     edit_image=Image.open(\"images.jpg\"),  \n",
    "#     seed=42,  \n",
    "#     num_inference_steps=40  \n",
    "# )    \n",
    "# image.save(\"test_output1.jpg\")\n",
    "\n",
    "\n",
    "# Path to LoRA checkpoints\n",
    "lora_dir = \"./models/train/ranveer_lora\"\n",
    "\n",
    "# Load edit image once\n",
    "edit_image = Image.open(\"rahul.jpeg\")\n",
    "\n",
    "# Loop through all .safetensors files in lora_dir\n",
    "for file_name in sorted(os.listdir(lora_dir)):\n",
    "    if file_name.endswith(\".safetensors\"):\n",
    "        lora_path = os.path.join(lora_dir, file_name)\n",
    "        print(f\"ðŸ”¹ Testing LoRA: {file_name}\")\n",
    "\n",
    "        # Load LoRA weights\n",
    "        pipe.load_lora(pipe.dit, lora_path, alpha=1.5)\n",
    "\n",
    "        output = pipe(\n",
    "            prompt=\"Replace the person face with Ranveer Singh\",\n",
    "            edit_image=edit_image,\n",
    "            cfg_scale=9.0,\n",
    "            seed=42,\n",
    "            num_inference_steps=60\n",
    "        )\n",
    "\n",
    "        # Save with unique filename\n",
    "        output_path = f\"rahul_outputs/{os.path.splitext(file_name)[0]}_output.jpg\"\n",
    "        os.makedirs(\"rahul_outputs\", exist_ok=True)\n",
    "        output.save(output_path)\n",
    "\n",
    "        print(f\"âœ… Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81b498ca-67ee-41ba-8ba6-57daa659cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /workspace/models/Qwen/Qwen-Image\n",
      "Loading models from: ['./models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00008-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00001-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00002-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00004-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00009-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00007-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00006-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00003-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00005-of-00009.safetensors']\n",
      "    model_name: qwen_image_dit model_class: QwenImageDiT\n",
      "    The following models are loaded: ['qwen_image_dit'].\n",
      "Downloading Model from https://www.modelscope.cn to directory: /workspace/models/Qwen/Qwen-Image\n",
      "Loading models from: ['./models/Qwen/Qwen-Image/text_encoder/model-00004-of-00004.safetensors', './models/Qwen/Qwen-Image/text_encoder/model-00001-of-00004.safetensors', './models/Qwen/Qwen-Image/text_encoder/model-00003-of-00004.safetensors', './models/Qwen/Qwen-Image/text_encoder/model-00002-of-00004.safetensors']\n",
      "    model_name: qwen_image_text_encoder model_class: QwenImageTextEncoder\n",
      "    The following models are loaded: ['qwen_image_text_encoder'].\n",
      "Downloading Model from https://www.modelscope.cn to directory: /workspace/models/Qwen/Qwen-Image\n",
      "Loading models from: ./models/Qwen/Qwen-Image/vae/diffusion_pytorch_model.safetensors\n",
      "    model_name: qwen_image_vae model_class: QwenImageVAE\n",
      "    The following models are loaded: ['qwen_image_vae'].\n",
      "Using qwen_image_text_encoder from ['./models/Qwen/Qwen-Image/text_encoder/model-00004-of-00004.safetensors', './models/Qwen/Qwen-Image/text_encoder/model-00001-of-00004.safetensors', './models/Qwen/Qwen-Image/text_encoder/model-00003-of-00004.safetensors', './models/Qwen/Qwen-Image/text_encoder/model-00002-of-00004.safetensors'].\n",
      "Using qwen_image_dit from ['./models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00008-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00001-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00002-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00004-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00009-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00007-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00006-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00003-of-00009.safetensors', './models/Qwen/Qwen-Image/transformer/diffusion_pytorch_model-00005-of-00009.safetensors'].\n",
      "Using qwen_image_vae from ./models/Qwen/Qwen-Image/vae/diffusion_pytorch_model.safetensors.\n",
      "No qwen_image_blockwise_controlnet models available.\n",
      "Downloading Model from https://www.modelscope.cn to directory: /workspace/models/Qwen/Qwen-Image\n",
      "720 tensors are updated by LoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:23<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "import sys, os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'  \n",
    "from diffsynth.pipelines.qwen_image import QwenImagePipeline, ModelConfig  \n",
    "from PIL import Image  \n",
    "import torch  \n",
    "  \n",
    "# Load base Qwen-Image model  \n",
    "pipe = QwenImagePipeline.from_pretrained(  \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    device=\"cuda\",  \n",
    "    model_configs=[  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\",   \n",
    "                   origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\",   \n",
    "                   origin_file_pattern=\"text_encoder/model*.safetensors\"),  \n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\",   \n",
    "                   origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),  \n",
    "    ],  \n",
    "    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image\",   \n",
    "                                 origin_file_pattern=\"tokenizer/\"),  \n",
    "    # Note: No processor_config needed for base model  \n",
    ")  \n",
    "  \n",
    "# Load your trained LoRA  \n",
    "pipe.load_lora(pipe.dit, \"./models/train/ranveer_lora/step-4000.safetensors\", alpha=1.5)  \n",
    "  \n",
    "# Generate new image  \n",
    "output = pipe(  \n",
    "    prompt=\"Ranveer Singh in a mahabharat\",  \n",
    "    cfg_scale=7.5,  # Base model uses default cfg_scale=4.0  \n",
    "    seed=42,  \n",
    "    num_inference_steps=40,  # Base model uses default 30 steps  \n",
    "    height=1328,  \n",
    "    width=1328  \n",
    ")  \n",
    "output.save(\"ranveer_singh_in_mahabharat.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5162b-0a6d-4c72-92d6-a10c431965cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
